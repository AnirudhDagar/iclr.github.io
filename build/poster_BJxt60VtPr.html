<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <title> ICLR: Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping </title>
</head>

  <body >
    
<!-- NAV -->

<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">

<style>
    body{font-family: 'Lato', sans-serif;}
</style>

<nav class="navbar navbar-expand-lg navbar-light bg-light mr-auto">
  <a class="navbar-brand" href="#">
    <img class="logo" src="https://www.iclr.cc/static/admin/img/ICLR-logo.png"  height="35"/>
  </a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">
        <a class="nav-link" href="index.html">Home</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="livestream.html">Live Stream</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="papers.html">Papers</a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="paper_vis.html">PaperVis</a>
      </li>










      </ul>
</div>
  </div>
</nav>


<div class="container">
  
 <!-- Title -->

<div class="card" >
  <div class="card-body">
    <h3 class="card-title">Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping</h3>
    <h6 class="card-subtitle mb-2 text-muted">
      Adam W. Harley,
      
      Shrinidhi K. Lakshmikanth,
      
      Fangyu Li,
      
      Xian Zhou,
      
      Hsiao-Yu Fish Tung,
      
      Katerina Fragkiadaki,
      
    </h6>
    <p class="card-text"><span class="font-weight-bold">TL;DR:</span> We show that with the right loss and architecture, view-predictive learning improves 3D object detection</p>
    <p class="card-text"><span class="font-weight-bold">Abstract:</span> Predictive coding theories suggest that the brain learns by predicting observations at various levels of abstraction. One of the most basic prediction tasks is view prediction: how would a given scene look from an alternative viewpoint? Humans excel at this task. Our ability to imagine and fill in missing information is tightly coupled with perception: we feel as if we see the world in 3 dimensions, while in fact, information from only the front surface of the world hits our retinas. This paper explores the role of view prediction in the development of 3D visual recognition. We propose neural 3D mapping networks, which take as input 2.5D (color and depth) video streams captured by a moving camera, and lift them to stable 3D feature maps of the scene, by disentangling the scene content from the motion of the camera. The model also projects its 3D feature maps to novel viewpoints, to predict and match against target views. We propose contrastive prediction losses to replace the standard color regression loss, and show that this leads to better performance on complex photorealistic data. We show that the proposed model learns visual representations useful for (1) semi-supervised learning of 3D object detectors, and (2) unsupervised learning of 3D moving object detectors, by estimating the motion of the inferred 3D feature maps in videos of dynamic scenes. To the best of our knowledge, this is the first work that empirically shows view prediction to be a scalable self-supervised task beneficial to 3D object detection. </p>
    
    <center>
      <a class="card-link"  href="http://www.openreview.net/pdf/abeec25f410ea4e54959aa68bc7ed0aa36858274.pdf">Paper</a>
      <a class="card-link"  href="http://www.openreview.net/forum?id=BJxt60VtPr">OpenReview</a>
    <!-- <span><a href="" class="btn btn-secondary">OpenReview</a></span> -->

      <a href="https://github.com/aharley/neural_3d_mapping" class="card-link">Code</a>
      <a href="" class="card-link">Slides</a>
    </center>
    <p></p>
    <p class="card-text"><span class="font-weight-bold">Keywords:</span>
      
      <a href="keyword_3D feature learning.html" class="text-secondary text-decoration-none">3D feature learning</a>,
      
      <a href="keyword_unsupervised learning.html" class="text-secondary text-decoration-none">unsupervised learning</a>,
      
      <a href="keyword_inverse graphics.html" class="text-secondary text-decoration-none">inverse graphics</a>,
      
      <a href="keyword_object discovery.html" class="text-secondary text-decoration-none">object discovery</a>,
      
    </p>    
  </div>
</div>

<div>
</div>

</div>


<!-- SlidesLive -->
<div id="presentation-embed-38915748" class="container container-sm"></div>
<script src='https://slideslive.com/embed_presentation.js'></script>
<script>
  embed = new SlidesLiveEmbed('presentation-embed-38915748', {
        presentationId: '38915748',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true
    });
</script>


<!-- Buttons -->
<div  class="container" style="padding-bottom: 30px; padding-top:30px">
<!--   <center> -->
<!--     <span><a href="poster_.html" class="btn btn-secondary">Prev</a></span> -->

<!--     <span><a href="" class="btn btn-secondary">Video Call</a></span> -->

<!--     <span><a href="poster_.html" class="btn btn-secondary">Next</a></span> </center> -->
<!-- </div> -->
<!--   <center> -->
<center>
    <h2> Paper Discussion       </h2>
    <span><a class="btn btn-secondary" href="https://gitter.im/iclr/posterBJxt60VtPr/">Chat</a></span>
</center>
<p></p>
<!-- Gitter -->
<div id="gitter" class="gitter container" height="600px">
  <center>
    <div class="border">
      <center> <iframe frameborder="0" src="https://gitter.im/iclr/posterBJxt60VtPr/~embed" width="900px" height="400px"></iframe> </center>
    </div>
  </center>
</div>

<!-- Recs -->
<p></p>
<div  class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Similar Papers </h2>
</div>
<p></p>

<div  class="container" >
  <div class="row">
    <div class="card-deck">

      
      
  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_rkl03ySYDH.html" class="text-dark"><h5 class="card-title">SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition</h5></a>      

    </div>
      <div class="card-body">
        <p class="card-text"> We propose a generative latent variable model for unsupervised scene decomposition that provides factorized object representation per foreground object while also decomposing background segments of complex morphology.</p>
      </div>
      
      <div class="card-footer">
      <center>
        <a href="poster_rkl03ySYDH.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  
      
  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_SkeyppEFvS.html" class="text-dark"><h5 class="card-title">CoPhy: Counterfactual Learning of Physical Dynamics</h5></a>      

    </div>
      <div class="card-body">
        <p class="card-text"> Understanding causes and effects in mechanical systems is an essential component of reasoning in the physical world. This work poses a new problem of counterfactual learning of object mechanics from visual input.  We develop the CoPhy benchmark to as...</p>
      </div>
      
      <div class="card-footer">
      <center>
        <a href="poster_SkeyppEFvS.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  
      
  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_Hyg9anEFPS.html" class="text-dark"><h5 class="card-title">Image-guided Neural Object Rendering</h5></a>      

    </div>
      <div class="card-body">
        <p class="card-text"> We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis while considering view-dependent effects.</p>
      </div>
      
      <div class="card-footer">
      <center>
        <a href="poster_Hyg9anEFPS.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  
      
  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_BkxfaTVFwH.html" class="text-dark"><h5 class="card-title">GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations</h5></a>      

    </div>
      <div class="card-body">
        <p class="card-text"> We present the first object-centric generative model of 3D visual scenes capable of both decomposing and generating scenes.</p>
      </div>
      
      <div class="card-footer">
      <center>
        <a href="poster_BkxfaTVFwH.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  
    </DIV>
          </DIV>
      </DIV>

</body>