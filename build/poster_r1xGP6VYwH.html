<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <title> ICLR: Optimistic Exploration even with a Pessimistic Initialisation </title>
</head>

  <body >
    
<!-- NAV -->

<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">

<style>
    body{font-family: 'Lato', sans-serif;}
</style>

<nav class="navbar navbar-expand-lg navbar-light bg-light mr-auto">
  <a class="navbar-brand" href="#">
    <img class="logo" src="https://www.iclr.cc/static/admin/img/ICLR-logo.png"  height="35"/>
  </a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">
        <a class="nav-link" href="index.html">Home</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="livestream.html">Live Stream</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="papers.html">Papers</a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="paper_vis.html">PaperVis</a>
      </li>










      </ul>
</div>
  </div>
</nav>


<div class="container">
  
 <!-- Title -->

<div class="card" >
  <div class="card-body">
    <h3 class="card-title">Optimistic Exploration even with a Pessimistic Initialisation</h3>
    <h6 class="card-subtitle mb-2 text-muted">
      Tabish Rashid,
      
      Bei Peng,
      
      Wendelin Boehmer,
      
      Shimon Whiteson,
      
    </h6>
    <p class="card-text"><span class="font-weight-bold">TL;DR:</span> We augment the Q-value estimates with a count-based bonus that ensures optimism during action selection and bootstrapping, even if the Q-value estimates are pessimistic.</p>
    <p class="card-text"><span class="font-weight-bold">Abstract:</span> Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.</p>
    
    <center>
      <a class="card-link"  href="http://www.openreview.net/pdf/4c77de9fe2127ab40ce5141bdcbc2d6f83494302.pdf">Paper</a>
      <a class="card-link"  href="http://www.openreview.net/forum?id=r1xGP6VYwH">OpenReview</a>
    <!-- <span><a href="" class="btn btn-secondary">OpenReview</a></span> -->

      <a href="" class="card-link">Code</a>
      <a href="" class="card-link">Slides</a>
    </center>
    <p></p>
    <p class="card-text"><span class="font-weight-bold">Keywords:</span>
      
      <a href="keyword_Reinforcement Learning.html" class="text-secondary text-decoration-none">Reinforcement Learning</a>,
      
      <a href="keyword_Exploration.html" class="text-secondary text-decoration-none">Exploration</a>,
      
      <a href="keyword_Optimistic Initialisation.html" class="text-secondary text-decoration-none">Optimistic Initialisation</a>,
      
    </p>    
  </div>
</div>

<div>
</div>

</div>


<!-- SlidesLive -->
<div id="presentation-embed-38915748" class="container container-sm"></div>
<script src='https://slideslive.com/embed_presentation.js'></script>
<script>
  embed = new SlidesLiveEmbed('presentation-embed-38915748', {
        presentationId: '38915748',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true
    });
</script>


<!-- Buttons -->
<div  class="container" style="padding-bottom: 30px; padding-top:30px">
<!--   <center> -->
<!--     <span><a href="poster_.html" class="btn btn-secondary">Prev</a></span> -->

<!--     <span><a href="" class="btn btn-secondary">Video Call</a></span> -->

<!--     <span><a href="poster_.html" class="btn btn-secondary">Next</a></span> </center> -->
<!-- </div> -->
<!--   <center> -->
<center>
    <h2> Paper Discussion       </h2>
    <span><a class="btn btn-secondary" href="https://gitter.im/iclr/posterr1xGP6VYwH/">Chat</a></span>
</center>
<p></p>
<!-- Gitter -->
<div id="gitter" class="gitter container" height="600px">
  <center>
    <div class="border">
      <center> <iframe frameborder="0" src="https://gitter.im/iclr/posterr1xGP6VYwH/~embed" width="900px" height="400px"></iframe> </center>
    </div>
  </center>
</div>

<!-- Recs -->
<p></p>
<div  class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Similar Papers </h2>
</div>
<p></p>

<div  class="container" >
  <div class="row">
    <div class="card-deck">

      
      
  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_rklHqRVKvH.html" class="text-dark"><h5 class="card-title">Harnessing Structures for Value-Based Planning and Reinforcement Learning</h5></a>      

    </div>
      <div class="card-body">
        <p class="card-text"> We propose a generic framework that allows for exploiting the low-rank structure in both planning and deep reinforcement learning.</p>
      </div>
      
      <div class="card-footer">
      <center>
        <a href="poster_rklHqRVKvH.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  
      
  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_S1xitgHtvS.html" class="text-dark"><h5 class="card-title">Making Sense of Reinforcement Learning and Probabilistic Inference</h5></a>      

    </div>
      <div class="card-body">
        <p class="card-text"> Popular algorithms that cast &#34;RL as Inference&#34; ignore the role of uncertainty and exploration. We highlight the importance of these issues and present a coherent framework for RL and inference that handles them gracefully.</p>
      </div>
      
      <div class="card-footer">
      <center>
        <a href="poster_S1xitgHtvS.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  
      
  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_BkxXe0Etwr.html" class="text-dark"><h5 class="card-title">CAQL: Continuous Action Q-Learning</h5></a>      

    </div>
      <div class="card-body">
        <p class="card-text"> A general framework of value-based reinforcement learning for continuous control</p>
      </div>
      
      <div class="card-footer">
      <center>
        <a href="poster_BkxXe0Etwr.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  
      
  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_BJewlyStDr.html" class="text-dark"><h5 class="card-title">On Bonus Based Exploration Methods In The Arcade Learning Environment</h5></a>      

    </div>
      <div class="card-body">
        <p class="card-text"> We find that existing bonus-based exploration methods have not been able to address the exploration-exploitation trade-off in the Arcade Learning Environment. </p>
      </div>
      
      <div class="card-footer">
      <center>
        <a href="poster_BJewlyStDr.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  
    </DIV>
          </DIV>
      </DIV>

</body>